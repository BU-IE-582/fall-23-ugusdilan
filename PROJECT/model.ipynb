{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86796462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e011b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, LSTM, GRU\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a528385",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class LSTMModel():\n",
    "\n",
    "    def __init__(self,Ntest,T,Nsplit,epochs, Layer1,Layer2,featureNumber,stockname):\n",
    "        #output number\n",
    "        self.Ntest=Ntest\n",
    "        #input number\n",
    "        self.Tx=T\n",
    "        #test-train split\n",
    "        self.Nsplit=Nsplit\n",
    "        self.epochs=epochs\n",
    "        self.Layer1=Layer1\n",
    "        self.Layer2=Layer2\n",
    "        self.featureNumber=featureNumber\n",
    "        self.stockname=stockname\n",
    "\n",
    "    def splitDataset(self,df):\n",
    "        train = df.iloc[:-self.Nsplit]\n",
    "        test = df.iloc[-self.Nsplit:]\n",
    "\n",
    "        # boolean series to index df rows\n",
    "        train_idx = df.index <= train.index[-1]\n",
    "        test_idx = df.index > train.index[-1]\n",
    "\n",
    "        return df,train,test,train_idx,test_idx\n",
    "\n",
    "    def supervisedDataset(self,df,T,Ntest,scaler):\n",
    "        # Scale the dataset\n",
    "\n",
    "        df_scaled = pd.DataFrame(scaler.fit_transform(df.values))\n",
    "        df_scaled.set_index(df.index,inplace=True)\n",
    "        scaler_name=\"minmax_scaler_{}\".format(self.stockname)\n",
    "\n",
    "\n",
    "        with open(scaler_name, 'wb') as f:\n",
    "            pickle.dump(scaler, f)\n",
    "\n",
    "        df_scaled['Seconds'] = df_scaled.index.map(pd.Timestamp.timestamp)\n",
    "        day = 60 * 60 * 24\n",
    "        year = 365.2425 * day\n",
    "\n",
    "        df_scaled['Day sin'] = np.sin(df_scaled['Seconds'] * (2 * np.pi / day))\n",
    "        df_scaled['Day cos'] = np.cos(df_scaled['Seconds'] * (2 * np.pi / day))\n",
    "        df_scaled['Year sin'] = np.sin(df_scaled['Seconds'] * (2 * np.pi / year))\n",
    "        df_scaled['Year cos'] = np.cos(df_scaled['Seconds'] * (2 * np.pi / year))\n",
    "        df_scaled.drop(columns={'Seconds'}, inplace=True)\n",
    "\n",
    "\n",
    "        Tx = T\n",
    "        Ty = Ntest\n",
    "        X = []\n",
    "        Y = []\n",
    "\n",
    "        # Make supervised dataset\n",
    "        series = df_scaled.dropna().to_numpy()\n",
    "        series_out=series[:,0]\n",
    "        for t in range(len(series) - Tx - Ty + 1):\n",
    "            x = series[t:t + Tx]\n",
    "            X.append(x)\n",
    "            y = series_out[t + Tx:t + Tx + Ty]\n",
    "            Y.append(y)\n",
    "\n",
    "        X = np.array(X).reshape(-1, Tx, self.featureNumber)\n",
    "        Y = np.array(Y).reshape(-1, Ty)\n",
    "\n",
    "        print(\"X.shape\", X.shape, \"Y.shape\", Y.shape)\n",
    "\n",
    "        Xtrain_m, Ytrain_m = X[:-self.Nsplit], Y[:-self.Nsplit]\n",
    "        Xtest_m, Ytest_m = X[-self.Nsplit:], Y[-self.Nsplit:]\n",
    "\n",
    "        return df_scaled,Xtrain_m,Xtest_m,Ytrain_m,Ytest_m\n",
    "\n",
    "    def build_model_rnn(self,Xtrain_m,Ytrain_m,Xtest_m,Ytest_m):\n",
    "\n",
    "        i = Input(shape=(self.Tx, self.featureNumber))\n",
    "        x = LSTM(self.Layer1, return_sequences=True)(i)\n",
    "        x = LSTM(self.Layer2, return_sequences=True)(x)\n",
    "        x = GlobalMaxPooling1D()(x)\n",
    "        x = Dense(self.Ntest)(x)\n",
    "        model = Model(i, x)\n",
    "\n",
    "        model_name=\"bestmodel_{}.h5\".format(self.stockname)\n",
    "        check_point = ModelCheckpoint(model_name, monitor='val_loss', save_best_only=True)\n",
    "\n",
    "        model.compile(\n",
    "            loss='mse',\n",
    "            optimizer='adam',\n",
    "        )\n",
    "\n",
    "        r = model.fit(\n",
    "            Xtrain_m,\n",
    "            Ytrain_m,\n",
    "            epochs=self.epochs,\n",
    "            validation_data=(Xtest_m, Ytest_m),\n",
    "            callbacks=[check_point],\n",
    "        )\n",
    "\n",
    "        plt.plot(r.history['loss'], label='train loss')\n",
    "        plt.plot(r.history['val_loss'], label='test loss')\n",
    "        plt.title(self.stockname)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # load best model\n",
    "        best_model = tf.keras.models.load_model(model_name)\n",
    "\n",
    "        return best_model\n",
    "\n",
    "    def predict(self,model, Xtrain, Xtest):\n",
    "        Ptrain = model.predict(Xtrain)\n",
    "        Ptest = model.predict(Xtest)\n",
    "\n",
    "        return Ptrain, Ptest\n",
    "\n",
    "    def evalutePredictions(self,Ptrain,Ptest,scaler, train_idx, test_idx,df,Ytest_m):\n",
    "\n",
    "        Ptrain_inverse = scaler.inverse_transform(Ptrain)\n",
    "        Ptest_inverse = scaler.inverse_transform(Ptest)\n",
    "        Ytest=scaler.inverse_transform(Ytest_m)\n",
    "        Y_test_mape=Ytest[:,0]\n",
    "        Ptest_mape=Ptest_inverse[:,0]\n",
    "\n",
    "        N=len(Ptrain)\n",
    "        train_idx[:self.Tx] = False\n",
    "        train_idx[(N+self.Ntest):]=False\n",
    "\n",
    "        test_idx[((N+self.Ntest)):]=True\n",
    "        test_idx[(-(self.Ntest-1)):] = False\n",
    "\n",
    "\n",
    "\n",
    "        df.loc[train_idx, 'multistep_train'] = Ptrain_inverse[:,0]\n",
    "        df.loc[test_idx, 'multistep_test'] = Ptest_inverse[:,0]\n",
    "\n",
    "        mape1 = mean_absolute_percentage_error(\n",
    "            df.loc[test_idx, 'price'], df.loc[test_idx, 'multistep_test'])\n",
    "        print(\"1-step MAPE:\", mape1 * 100)\n",
    "\n",
    "        mape2 = mean_absolute_percentage_error(\n",
    "            Y_test_mape, Ptest_mape)\n",
    "        print(\"1-step MAPE:\", mape2 * 100)\n",
    "        return df,mape1*100\n",
    "\n",
    "    def dfModel(self,df,resolution):\n",
    "        df = df.dropna()\n",
    "\n",
    "        # scaling\n",
    "        # Create a MinMaxScaler instance\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "        df_first,train, test, train_idx, test_idx = self.splitDataset(df)\n",
    "\n",
    "        df,Xtrain_m, Xtest_m, Ytrain_m, Ytest_m=self.supervisedDataset(df,self.Tx,self.Ntest,scaler)\n",
    "\n",
    "        best_model=self.build_model_rnn(Xtrain_m, Ytrain_m, Xtest_m, Ytest_m)\n",
    "\n",
    "        Ptrain, Ptest=self.predict(best_model,Xtrain_m,Xtest_m)\n",
    "\n",
    "        df,mape=self.evalutePredictions(Ptrain,Ptest,scaler, train_idx, test_idx,df_first,Ytest_m)\n",
    "\n",
    "\n",
    "        return df,mape,Ytest_m,Ptest"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
